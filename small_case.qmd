---
title: "Small Case: Checking Hitting Times in the Year of Our Lord"
format:
  html:
    code-fold: true
---

```{python Imports} 
import numpy as np
import networkx as nx
import igraph as ig
import polars as pl

import hitting_markov as hmk
import hitting_time_analysis as ht
import hitting_time_multichain as mch
import network_conversion as ntc

from matplotlib import pyplot as plt, colormaps
import seaborn as sns
```

Here we want to check that our simulations and analytical results have parity.

The idea is to setup a small network that has at least one very rare link, and compute the hitting times analytically for N=[1, 5, 10, 30] individuals, and also simualte for those cases.

We generate a small 5-node directed, weighted network.
```{python}
G = ig.Graph(
  edges = [
    (0, 1),
    (0, 3),
    (1, 2),
    (1, 3),
    (2, 0),
    (3, 2),
    (3, 4),
    (4, 3),
  ],
  directed = True,
  vertex_attrs = {
    'name': list('ABCDE'),
    'capacity': [100, 120, 200, 100, 30],
  },
  edge_attrs = {
    'weight': [
      300, # (0, 1),
      50, # (0, 3),
      50, # (1, 2),
      100, # (1, 3),
      150, # (2, 0),
      120, # (3, 2),
      60, # (3, 4),
      10, # (4, 3),
    ]
  }
)

G.write_graphml("small_case/graph.graphml")

fig, ax = plt.subplots()
ig.plot(
  G,
  target = ax,
  vertex_label=G.vs['name'],
  vertex_color='lightcyan',
  vertex_size=50,
  edge_label=G.es['weight'],
  edge_background='white',
  edge_label_dist=1.7,
)
```

We compute Q, the transition matrix.
We extract T, the base transition matrix, by allowing defaults of using the vertex indices as the ordering, since we have the sizes attached to the graph.
```{python}
TIMESPAN = 28
T = ntc.transition_matrix_from_graph(
  graph=G,
  scaling_per_node=G.vs['capacity'],
  global_scaling=TIMESPAN,
  adjacency_attribute='weight'
)
Q = hmk._Q_mat_naive(T)
```

This is our ground truth, the hitting time for N=1, as extracted from the linear system that results.
```{python}
def draw_ecdf(values, ax=None, **kwargs):
  value_arr = np.sort(values.flatten())
  ecdf_quantiles = np.linspace(0, 1, len(value_arr), endpoint=False)

  if ax is None:
    fig, ax = plt.subplots()
  ax.step(value_arr, ecdf_quantiles, **kwargs)
  return ax
```

```{python}
hitting_time_one,_ = ht.compute_static_hitting_times(Q)
draw_ecdf(hitting_time_one)
```

Here we use a scaling=100 to get proper convergence of the integrals. This value is relatively arbitrary, but it must be >1 since there are slow species that ruin our time.
```{python}
hitting_time_numerical = mch.compute_all_multichain_hittings(1, Q=Q, scaling=100)
ax = draw_ecdf(hitting_time_numerical)
```

These are our numerical base results. We "hope" they are correct (insofar as numerical error)
```{python}
ht_numers = {
  n_chains: mch.compute_all_multichain_hittings(n_chains, Q=Q, scaling=100)
  for n_chains in [1, 5, 10, 30]
}

fig, ax = plt.subplots()
for k, v in ht_numers.items():
  draw_ecdf(v, ax=ax, label=f"N={k}")
plt.legend()
```

We can then simulate on this naive static network.
We will offload this work to another script, mostly because the simulations take a significant amount of time (and we only want to run it once)

[script](small_case_sims.py)

Similarly, we offload the construction of the metrics.

[script](small_case_sim_analysis.py)

Here, we construct the empirical CDF
```{python}
metrics = pl.read_parquet("small_case/metrics.parquet")
hitting_time_cols = pl.selectors.starts_with('hitting_time_')
aggregated_metrics = metrics.unpivot(
  on=hitting_time_cols,
  index=['nchains','seed',],
  variable_name='target',
  value_name='hitting_time',
)
max_value_plus_one = aggregated_metrics.select(pl.col('hitting_time').max()).item() + 1
cleaned_metrics = aggregated_metrics.fill_nan(max_value_plus_one).fill_null(max_value_plus_one)

ax = sns.ecdfplot(cleaned_metrics, x='hitting_time', hue='nchains', palette=colormaps.get('jet'))

for k, v in ht_numers.items():
  draw_ecdf(v, ax=ax, label=f"N={k}")

ax.set_xscale('log')
ax.set_xlim(left=0.3, right=3e3)
```

We see relatively good agreement here.
<!-- We detected a bug in this sequence, where the silent graceful failure of numpy.random.Generator.multinomial causes a bug where the transition probabilities that were not normalised correctly earlier cause high probability moves to the "last" node. -->